{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CA02-WallyDryden.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jzzHhQYASPs"
      },
      "source": [
        "Import os and numpy libraries. Mount local Gdrive folder in Colabs for data access."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uk8YWfXr5s0I",
        "outputId": "d24c112e-96e5-4fd8-bbdc-f4e0027cbfc6"
      },
      "source": [
        "import os\r\n",
        "import numpy as np  # load numpy\r\n",
        "from google.colab import drive  # load google colab for mounting gdrive\r\n",
        "drive.mount('/content/drive')  # mount gdrive to access emails\r\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh8mJQN0AueP"
      },
      "source": [
        "Import Counter from collections. Import GaussianNB and accuracy_score from sklearn library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FZZT_bN2xsK"
      },
      "source": [
        "from collections import Counter\r\n",
        "from sklearn.naive_bayes import GaussianNB\r\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWuumQHJAQ8L"
      },
      "source": [
        "Define a function that creates a list of all words (all_words) found in emails using iterators by email and then by line. Use the split function to divide lines into words to add to all_words. Next a dictionary is created using Counter() from all_words. Next non-alpha characters are removed and single characters are removed from the dictionary. Finally a list of the 3000 most common words are kept and the rest are discarded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgI1nu2F29Ea"
      },
      "source": [
        "def make_Dictionary(root_dir):\r\n",
        "  all_words = []\r\n",
        "  emails = [os.path.join(root_dir,f) for f in os.listdir(root_dir)]\r\n",
        "  for mail in emails:\r\n",
        "    with open(mail) as m:\r\n",
        "      for line in m:\r\n",
        "        words = line.split()\r\n",
        "        all_words += words\r\n",
        "  dictionary = Counter(all_words)\r\n",
        "  list_to_remove = list(dictionary)\r\n",
        "\r\n",
        "  for item in list_to_remove:\r\n",
        "    if item.isalpha() == False:\r\n",
        "      del dictionary[item]\r\n",
        "    elif len(item) == 1:\r\n",
        "      del dictionary[item]\r\n",
        "  dictionary = dictionary.most_common(3000)\r\n",
        "  return dictionary"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9I55A3ncDQWX"
      },
      "source": [
        "Define a function that creates a word frequency matrix (702 X 3000). A frequency of the words is given for the top 3000 words by email.  Each of the 703 rows represents one email in the training data.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1TL1_XT3RD0"
      },
      "source": [
        "\r\n",
        "def extract_features(mail_dir):\r\n",
        "  files = [os.path.join(mail_dir,fi) for fi in os.listdir(mail_dir)]\r\n",
        "  features_matrix = np.zeros((len(files),3000))\r\n",
        "  train_labels = np.zeros(len(files))\r\n",
        "  count = 1;\r\n",
        "  docID = 0;\r\n",
        "  for fil in files:\r\n",
        "    with open(fil) as fi:\r\n",
        "      for i, line in enumerate(fi):\r\n",
        "        if i ==2:\r\n",
        "          words = line.split()\r\n",
        "          for word in words:\r\n",
        "            wordID = 0\r\n",
        "            for i, d in enumerate(dictionary):\r\n",
        "              if d[0] == word:\r\n",
        "                wordID = i\r\n",
        "                features_matrix[docID,wordID] = words.count(word)\r\n",
        "      train_labels[docID] = 0;\r\n",
        "      filepathTokens = fil.split('/')\r\n",
        "      lastToken = filepathTokens[len(filepathTokens)-1]\r\n",
        "      if lastToken.startswith(\"spmsg\"):\r\n",
        "        train_labels[docID] = 1;\r\n",
        "        count = count + 1\r\n",
        "      docID = docID + 1\r\n",
        "  return features_matrix, train_labels\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFOZ8Po5Ik86"
      },
      "source": [
        "Set up file sources using Gdrive path plus custom folder names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6fAkDiH3V7S"
      },
      "source": [
        "TRAIN_DIR = '/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/train-mails'\r\n",
        "TEST_DIR = '/content/drive/My Drive/MSBA_Colab_2020/ML_Algorithms/CA02/Data/test-mails'"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rv2Ty-0Iwc4"
      },
      "source": [
        "Here the 3000 word dictionary created using TRAIN_DIR email files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCsEFAPZ7EnN"
      },
      "source": [
        "dictionary = make_Dictionary(TRAIN_DIR)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrYYB_b5I6wl"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIQtpyemI87-"
      },
      "source": [
        "Using a Gaussian statistics the likelyhood that an email is spam vs. non-spam is determined using the training emails. A model is created statistically that differentiates between spam and non-spam emails."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fe2vANuC8B4z",
        "outputId": "473bac35-7183-41e2-ba95-83ed296d2824"
      },
      "source": [
        "print (\"reading and processing emails from TRAIN and TEST folders\")\r\n",
        "features_matrix, labels = extract_features(TRAIN_DIR)\r\n",
        "test_features_matrix, test_labels = extract_features(TEST_DIR)\r\n",
        "\r\n",
        "model = GaussianNB()\r\n",
        "print (\"Training Model using Gaussian Naibe Bayes algorithm .....\")\r\n",
        "model.fit(features_matrix, labels)\r\n",
        "print (\"Training completed\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "reading and processing emails from TRAIN and TEST folders\n",
            "Training Model using Gaussian Naibe Bayes algorithm .....\n",
            "Training completed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6KgSWJNJfeQ"
      },
      "source": [
        "Model created above is tested vs. sample known emails and accuarcy of model is calculated."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUbbyS6X8xKq",
        "outputId": "4a63f2f6-90d9-4209-a4cd-98054d8b121b"
      },
      "source": [
        "\r\n",
        "\r\n",
        "print (\"testing trained model to predict Test Data labels\")\r\n",
        "predicted_labels = model.predict(test_features_matrix)\r\n",
        "print (\"Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\")\r\n",
        "print (accuracy_score(test_labels, predicted_labels))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "testing trained model to predict Test Data labels\n",
            "Completed classification of the Test Data .... now printing Accuracy Score by comparing the Predicted Labels with the Test Labels:\n",
            "0.9615384615384616\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}